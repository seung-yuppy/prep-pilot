import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer
from dotenv import load_dotenv
import os

# ğŸ” í™˜ê²½ ë³€ìˆ˜ì—ì„œ HuggingFace í† í° ë¡œë”©
load_dotenv()
HF_TOKEN = os.getenv("HF_TOKEN")

# ğŸ“Œ ëª¨ë¸ ì •ë³´
MODEL_DIR = "nlpai-lab/KULLM3"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# LLM ì €ì¥ ìœ„ì¹˜
model = AutoModelForCausalLM.from_pretrained(
    MODEL_DIR,
    torch_dtype=torch.float16,
    device_map="auto",  # ìë™ ë””ë°”ì´ìŠ¤ ë¶„ë°°
    offload_folder="offload"  # offload ì‹œ ì‚¬ìš©í•  í´ë” ì§€ì •
)

tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)

# âœ… ëª¨ë¸ & í† í¬ë‚˜ì´ì € ë¡œë“œ (Private ëª¨ë¸ì¼ ê²½ìš° token ê¼­ í¬í•¨)
tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, token=HF_TOKEN)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_DIR,
    torch_dtype=torch.float16 if DEVICE == "cuda" else torch.float32,
    device_map="auto",
    token=HF_TOKEN
)

# ğŸ’¬ ì‹¤ì‹œê°„ ì‘ë‹µ ì¶œë ¥ìš© ìŠ¤íŠ¸ë¦¬ë¨¸
streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)

# ğŸ§  LLM ì‹¤í–‰ í•¨ìˆ˜
def run_llm_review(user_input: str):
    system_prompt = "ë‹¹ì‹ ì€ í•œêµ­ì–´ êµìœ¡ìš© AIì…ë‹ˆë‹¤. ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë¬¸ì¥ì—ì„œ ë¬¸ë²• ì˜¤ë¥˜, ì˜¤íƒ€, ê°œë… ì˜¤ë¥˜ ë“±ì„ ì°¾ì•„ ì„¤ëª…í•´ì£¼ì„¸ìš”. ê°€ëŠ¥í•˜ë©´ ì‹ ë¢°ë„ë„ íŒë‹¨í•´ì£¼ì„¸ìš”."
    
    # ğŸ—£ï¸ Chat template êµ¬ì„±
    conversation = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_input}
    ]

    # ğŸ§¾ í…œí”Œë¦¿ ì ìš© ë° í† í°í™”
    inputs = tokenizer.apply_chat_template(
        conversation,
        tokenize=True,
        add_generation_prompt=True,
        return_tensors="pt"
    ).to(DEVICE)

    # ğŸš€ ì‘ë‹µ ìƒì„±
    _ = model.generate(
        inputs,
        streamer=streamer,
        max_new_tokens=1024,
        temperature=0.7,
        do_sample=True,
        top_p=0.95,
    )

# ğŸ–¥ï¸ ì§„ì…ì 
if __name__ == "__main__":
    user_input = input("ê²€ì‚¬í•  ë¬¸ì¥ì„ ì…ë ¥í•˜ì„¸ìš”: ")
    run_llm_review(user_input)
