import os
import json
from dotenv import load_dotenv
from transformers import AutoTokenizer, AutoModel, TextStreamer
import torch
from sklearn.metrics.pairwise import cosine_similarity
from tqdm import tqdm

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
HF_TOKEN = os.getenv("HF_TOKEN")
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# ëª¨ë¸ ì„¤ì •
MODEL_NAME = "snunlp/KR-SBERT-V40K-klueNLI-augSTS"
LLM_NAME = "nlpai-lab/kullm3-7b-instruct"  # ëª¨ë¸ëª…

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=HF_TOKEN)
model = AutoModel.from_pretrained(MODEL_NAME, token=HF_TOKEN).to(DEVICE)

# LLMìš©
llm_tokenizer = AutoTokenizer.from_pretrained(LLM_NAME, token=HF_TOKEN)
llm_model = AutoModel.from_pretrained(LLM_NAME, device_map="auto", torch_dtype=torch.float16, token=HF_TOKEN)
streamer = TextStreamer(llm_tokenizer)

# ìœ ì‚¬ë„ ê²€ì‚¬
def get_top_similar_chunks(user_input, embedded_chunks, top_k=3):
    input_emb = model.encode(user_input, convert_to_tensor=True).cpu().detach().numpy().reshape(1, -1)
    chunk_vectors = [chunk['vector'] for chunk in embedded_chunks]
    similarities = cosine_similarity(input_emb, chunk_vectors)[0]
    top_indices = similarities.argsort()[-top_k:][::-1]
    return [(embedded_chunks[i], similarities[i]) for i in top_indices]

# LLM í‰ê°€ í•¨ìˆ˜
def ask_llm(system_prompt, user_prompt):
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]
    input_ids = llm_tokenizer.apply_chat_template(messages, return_tensors="pt").to(DEVICE)
    output = llm_model.generate(input_ids, max_new_tokens=512, streamer=streamer, do_sample=False)
    return llm_tokenizer.decode(output[0], skip_special_tokens=True)

#  ë©”ì¸
def main():
    user_input = input("âœï¸ ë¬¸ì¥ì„ ì…ë ¥í•˜ì„¸ìš”: ")

    # 1. Load Embedding DB
    with open("data/embeddings/embedding_rebeccacho_gitbook.json", "r", encoding="utf-8") as f:
        embedded_chunks = json.load(f)

    # 2. ìœ ì‚¬ chunk ë½‘ê¸°
    top_chunks = get_top_similar_chunks(user_input, embedded_chunks, top_k=3)

    print("\nğŸ“Š ìœ ì‚¬í•œ ë¬¸ì¥:")
    for i, (chunk, sim) in enumerate(top_chunks):
        print(f"# {i+1} - ìœ ì‚¬ë„: {round(sim, 3)}\në³¸ë¬¸: {chunk['text']}\n")

    # 3. ì‹ ë¢°ë„ íŒë‹¨ (LLM)
    context = "\n".join([f"{i+1}. {chunk['text']}" for i, (chunk, _) in enumerate(top_chunks)])
    system_prompt = "ë„ˆëŠ” ìë°”ë¥¼ ê³µë¶€í•˜ëŠ” í•œêµ­ ë¸”ë¡œê·¸ ê¸€ì˜ ì‚¬ì‹¤ì„± íŒë‹¨ì„ ë•ëŠ” AIì•¼."
    user_prompt = f"ë‹¤ìŒ ë¬¸ì¥ì€ ì‹ ë¢°í•  ìˆ˜ ìˆì„ê¹Œ?\n\në¬¸ì¥: {user_input}\n\në¹„êµ ëŒ€ìƒ:\n{context}\n\nì‹ ë¢°ë„ í‰ê°€ì™€ ì´ìœ ë¥¼ ì•Œë ¤ì¤˜."

    print("\nğŸ¤– ì‹ ë¢°ë„ íŒë‹¨:")
    result = ask_llm(system_prompt, user_prompt)
    print(result)

    # 4. ì˜¤íƒ€ ë° ë¬¸ë²• ê²€ì‚¬
    correction_prompt = f"ë‹¤ìŒ ë¬¸ì¥ì—ì„œ ë¬¸ë²•ì  ì˜¤ë¥˜ë‚˜ ì˜¤íƒ€ê°€ ìˆë‹¤ë©´ ìˆ˜ì •í•˜ê³ , ì—†ë‹¤ë©´ 'ë¬¸ì œ ì—†ìŒ'ì´ë¼ê³  ë§í•´ì¤˜.\n\në¬¸ì¥: {user_input}"
    print("\nâœï¸ ì˜¤íƒ€ ê²€ì‚¬ ë° ìˆ˜ì •:")
    correction = ask_llm(system_prompt, correction_prompt)
    print(correction)

if __name__ == "__main__":
    main()
